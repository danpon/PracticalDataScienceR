---
title: "Méthodes d'apprentissage non-supervisé"
author: "Daniel Pont"
date: "06/07/2020"
output:
  html_document:
    theme: united
    toc: yes
    toc_depth: 4
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Méthodes d'apprentissage non-supervisé

## 1. Clustering

### 1.1 Distances

Distances courantes :

* Euclidienne
* Hamming (siatnce entre '101' et '110' = nbre digits différents = 2)
* Manhattan (city block : nbre de blocs horiz + nbre bloc vertic) 
* Cosinus ( Produit scalaire / Produit des normes)

### 1.2 Préparation des données

```{r 1.2, warning=FALSE,message=FALSE}
protein <- read.table("data/protein.txt", sep = "\t", header=TRUE)
summary(protein)

# mise à l'échelle

vars_to_use <- colnames(protein)[-1]
pmatrix <- scale(protein[, vars_to_use])
pcenter <- attr(pmatrix, "scaled:center")
pscale <- attr(pmatrix, "scaled:scale")

#supprime les attributs de mise à l'échelle
rm_scales <- function(scaled_matrix) {
    attr(scaled_matrix, "scaled:center") <- NULL
    attr(scaled_matrix, "scaled:scale") <- NULL
    scaled_matrix
}

pmatrix <- rm_scales(pmatrix)
```

### 1.3 Clustering hiérarchique 

#### 1.3.1 Création d'un dendrogramme avec hclust

```{r 1.3.1, warning=FALSE,message=FALSE}
distmat <- dist(pmatrix, method = "euclidean")
pfit <- hclust(distmat, method = "ward.D")
plot(pfit, labels = protein$Country)

#Extraction  des membres de chaque cluster
groups <- cutree(pfit, k = 5)

print_clusters <- function(data, groups, columns) {
    groupedD <- split(data, groups)
    lapply(groupedD,
    function(df) df[, columns])
}

cols_to_print <- wrapr::qc(Country, RedMeat, Fish, Fr.Veg)
print_clusters(protein, groups, cols_to_print)

```

#### 1.3.2 Projection des clusters sur les 2 principaux composants PCA :

```{r 1.3.2, warning=FALSE,message=FALSE}
library(ggplot2)

princ <- prcomp(pmatrix)
nComp <- 2

project <- predict(princ, pmatrix)[, 1:nComp]

project_plus <- cbind(as.data.frame(project),
                        cluster = as.factor(groups),
                        country = protein$Country)

ggplot(project_plus, aes(x = PC1, y = PC2)) +
        geom_point(data = as.data.frame(project), color = "darkgrey") +
        geom_point() +
        geom_text(aes(label = country),
        hjust = 0, vjust = 1) +
        facet_wrap(~ cluster, ncol = 3, labeller = label_both)
```

#### 1.3.3 Evaluation bootstrap des clusters - fpc::clusterboot

```{r 1.3.3, warning=FALSE,message=FALSE}
library(fpc)

kbest_p <- 5
cboot_hclust <- clusterboot(pmatrix,
                    clustermethod = hclustCBI,
                    method = "ward.D",
                    k = kbest_p)

summary(cboot_hclust$result)

groups <- cboot_hclust$result$partition
print_clusters(protein, groups, cols_to_print)

# Vector indiquant la stabilité des clusters
cboot_hclust$bootmean

# Le nombre de fois où chaque cluster a disparu (par défaut clusterboot() a 100 itérations)
cboot_hclust$bootbrd
```

#### 1.3.4 Détermination du nombre de clusters

Une 1ère solution est de calculer le total des  **within sum of squares (WSS)** de chque cluster pour différentes valeurs de k et de chercher un coude dans la courbe.

```{r 1.3.4.1 , warning=FALSE,message=FALSE}
sqr_edist <- function(x, y) {
    sum((x - y)^2)
}

# Fonction pour calculer le WSS d'un cluster
wss_cluster <- function(clustermat) {
    # centre du cluster
    c0 <- colMeans(clustermat)
    # calcule la distance au carré entre chaque point du cluster et le centre
    # calcule la somme  de ces distances au carré
    sum(apply(clustermat, 1, FUN = function(row) { sqr_edist(row, c0) }))
}

wss_total <- function(dmatrix, labels) {
    wsstot <- 0
    k <- length(unique(labels))
    for(i in 1:k)
        wsstot <- wsstot + wss_cluster(subset(dmatrix, labels == i))
    wsstot
}

# calcule la somme de tous les WSS
wss_total(pmatrix, groups)
```

Représentation graphique des WSS pour différentes valeurs de k :

```{r 1.3.4.2 , warning=FALSE,message=FALSE}
get_wss <- function(dmatrix, max_clusters) {
    wss = numeric(max_clusters)
    
    #wss de toutes les données (1 seul cluster)
    wss[1] <- wss_cluster(dmatrix)

    d <- dist(dmatrix, method = "euclidean")
    
    # clusterise les données
    pfit <- hclust(d, method = "ward.D")

    for(k in 2:max_clusters) {
        labels <- cutree(pfit, k = k)
        wss[k] <- wss_total(dmatrix, labels)
    }

    wss
}

kmax <- 10
cluster_meas <- data.frame(nclusters = 1:kmax,
                    wss = get_wss(pmatrix, kmax))

breaks <- 1:kmax
ggplot(cluster_meas, aes(x=nclusters, y = wss)) +
    geom_point() + geom_line() +
    scale_x_continuous(breaks = breaks)
```

Il n'y a pas de  coude net, les valeurs k=2,5 et 6 sont plausibles.

**Calinski-Harabasz index**

L'indice de Calinski-Harabasz est une autre mesure couramment utilisée de la qualité des Clusters.  Pour motiver (et calculer) l'indice de Calinski-Harabasz (indice CH, pour faire court), nous devons d'abord définir quelques termes supplémentaires.

* **TSS (Total Sum of Square)** d'un ensemble de points est la somme des distances au carré de tous les points du centre de gravité des données (WSS pour 1 seul cluster)

* **BSS (between sum of squares)** = TSS - WSS

Représentation graphique des BSS et WSS pour différentes valeurs de k :

```{r 1.3.4.3 , warning=FALSE,message=FALSE}
total_ss <- function(dmatrix) {
    grandmean <- colMeans(dmatrix)
    sum(apply(dmatrix, 1, FUN = function(row) { sqr_edist(row, grandmean) }))
}

tss <- total_ss(pmatrix)
cluster_meas$bss <- with(cluster_meas, tss - wss)

library(cdata)
cmlong <- unpivot_to_blocks(cluster_meas,
                            nameForNewKeyColumn = "measure",
                            nameForNewValueColumn = "value",
                            columnsToTakeFrom = c("wss", "bss"))

ggplot(cmlong, aes(x = nclusters, y = value)) +
        geom_point() + geom_line() +
        facet_wrap(~measure, ncol = 1, scale = "free_y") +
        scale_x_continuous(breaks = 1:10)
```

* within cluster variance W = WSS / (n - k)
* between cluster variance B = BSS / (k - 1)
* **Calinski-Harabasz (CH) index** = B/W

```{r 1.3.4.4 , warning=FALSE,message=FALSE}
cluster_meas$B <- with(cluster_meas, bss / (nclusters - 1))
n = nrow(pmatrix)
cluster_meas$W <- with(cluster_meas, wss / (n - nclusters))

# Calcul du CH
cluster_meas$ch_crit <- with(cluster_meas, B / W)

# Représentation graphique du CH
ggplot(cluster_meas, aes(x = nclusters, y = ch_crit)) +
        geom_point() + geom_line() +
        scale_x_continuous(breaks = 1:kmax)
```

### 1.4 K-means

#### 1.4.1 Exécution de K-means

```{r 1.4.1 , warning=FALSE,message=FALSE}
kbest_p <- 5
pclusters <- kmeans(pmatrix, kbest_p, nstart = 100, iter.max = 100)
summary(pclusters)

pclusters$centers

pclusters$size

groups <- pclusters$cluster
cols_to_print = wrapr::qc(Country, RedMeat, Fish, Fr.Veg)
print_clusters(protein, groups, cols_to_print)
```

#### 1.4.2 Détermination de K avec kmeansruns()

```{r 1.4.2 , warning=FALSE,message=FALSE}
clustering_ch <- kmeansruns(pmatrix, krange = 1:10, criterion = "ch")
clustering_ch$bestk

#asw = average silhoute weight
clustering_asw <- kmeansruns(pmatrix, krange = 1:10, criterion = "asw")
clustering_asw$bestk

clustering_asw$crit

clustering_ch$crit

cluster_meas$ch_crit

summary(clustering_ch)
```

#### 1.4.3 Utilisation de clusterboot() avec k-means

```{r 1.4.3 , warning=FALSE,message=FALSE}
kbest_p <- 5
cboot <- clusterboot(pmatrix, clustermethod = kmeansCBI,
                      runs = 100,iter.max = 100,
                      krange = kbest_p, seed = 15555)

groups <- cboot$result$partition

print_clusters(protein, groups, cols_to_print)
```

### 1.5 Assignation de nouveaux points aux clusters

#### 1.5.1 Fonction permettant d'assigner des points à un cluster

```{r 1.5.1 , warning=FALSE,message=FALSE}
assign_cluster <- function(newpt, centers, xcenter = 0, xscale = 1) {
                      xpt <- (newpt - xcenter) / xscale
                      dists <- apply(centers, 1, FUN = function(c0) { sqr_edist(c0, xpt) })
                      which.min(dists)
                  }
```

#### 1.5.2 Génération artificielle de clusters

```{r 1.5.2 , warning=FALSE,message=FALSE}
mean1 <- c(1, 1, 1)
sd1 <- c(1, 2, 1)
mean2 <- c(10, -3, 5)
sd2 <- c(2, 1, 2)
mean3 <- c(-5, -5, -5)
sd3 <- c(1.5, 2, 1)

library(MASS)
clust1 <- mvrnorm(100, mu = mean1, Sigma = diag(sd1))
clust2 <- mvrnorm(100, mu = mean2, Sigma = diag(sd2))
clust3 <- mvrnorm(100, mu = mean3, Sigma = diag(sd3))

toydata <- rbind(clust3, rbind(clust1, clust2))
tmatrix <- scale(toydata)
tcenter <- attr(tmatrix, "scaled:center")
tscale <-attr(tmatrix, "scaled:scale")
tmatrix <- rm_scales(tmatrix)

kbest_t <- 3
tclusters <- kmeans(tmatrix, kbest_t, nstart = 100, iter.max = 100)
tclusters$size
```


#### 1.5.3 Restitution des centres à l'état avant la mise à l'échelle

```{r 1.5.3 , warning=FALSE,message=FALSE}
unscaled = scale(tclusters$centers, center = FALSE, scale = 1 / tscale)
rm_scales(scale(unscaled, center = -tcenter, scale = FALSE))
```

#### Exemple d'assignation de points aux clusters

```{r 1.5.4 , warning=FALSE,message=FALSE}
assign_cluster(mvrnorm(1, mean1, diag(sd1)),
          tclusters$centers,
          tcenter, tscale)

assign_cluster(mvrnorm(1, mean2, diag(sd2)),
          tclusters$centers,
          tcenter, tscale)


assign_cluster(mvrnorm(1, mean3, diag(sd3)),
          tclusters$centers,
          tcenter, tscale)
```

## 2. Règles d'association

### 2.1 Introduction

Les règles d'association sont utilisée pour rechercher des objets ou des attributs qui apparaissent fréquemment ensemble, par exemple, des produits qui sont souvent achetés simultanemént lors d'une session de shopping ou des requêtes qui ont tendance à être exécutées dans une même session sur le moteur de recherche d'un site Web. Ces informations peuvent être utilisées pour recommander des produits aux acheteurs, pour regrouper les articles fréquemment regroupés sur les étagères des magasins ou pour repenser les sites Web pour une navigation plus facile.

Les éléments clés sont les suivants :

* **Règles** 

La règle «si X, alors Y» signifie que chaque fois que vous voyez l'ensemble d'éléments X dans une transaction, vous vous attendez à voir également Y (avec une confiance donnée). Pour l'algorithme apriori (que nous verrons dans cette section), Y est toujours un ensemble avec un seul élément.


* **Support** : 

Supposons que votre base de données de transactions s'appelle T et X est un ensemble d'éléments. Le support (X) est le nombre de transactions qui contiennent X divisé par le nombre total de transactions dans T.

* **Indice de Confiance** :

La confiance d'une régle est définit par la proportion de transactions de T contenat X qui contiennent aussi Y.. Le but de l'exploration de règles d'association est de trouver toutes les règles intéressantes dans la base de données avec au moins un support minimum donné (disons, 10%) et une confiance donnée minimum (disons, 60%)

### 2.2 Exemple

Supposons que vous travailliez pour une librairie et que vous souhaitiez recommander des livres qui pourraient intéresser un client, en fonction de tous ses achats et des livres précédemment consultés. Vous souhaitez utiliser les informations de l'historique pour développer des règles de recommandation.

### 2.3 Utilisation du package arules

#### 2.3.1 Lecture des données

```{r 2.3.1 , warning=FALSE,message=FALSE}
library(arules)
bookbaskets <- read.transactions("data/bookdata.tsv.gz",
                  format = "single",
                  header = TRUE,
                  sep = "\t",
                  cols = c("userid", "title"),
                  rm.duplicates = TRUE)
```

NB : cols spécifie respectivement les colonnes *id de transaction* et *id d'élement*

#### 2.3.2 Exploration des données

Données de transaction:

```{r 2.3.2.1 , warning=FALSE,message=FALSE}
class(bookbaskets)
bookbaskets
dim(bookbaskets)
# titre des livres
colnames(bookbaskets)[1:5]
# id des clients
rownames(bookbaskets)[1:5]

basketSizes <- size(bookbaskets)
summary(basketSizes)
```

Distribution de la taille des paniers :

```{r 2.3.2.2 , warning=FALSE,message=FALSE}
library(ggplot2)

quantile(basketSizes, probs = seq(0, 1, 0.1))

ggplot(data.frame(count = basketSizes)) +
        geom_density(aes(x = count)) +
        scale_x_log10()
```

Fréquence d'apparition des livres :

```{r 2.3.2.3 , warning=FALSE,message=FALSE}
bookCount <- itemFrequency(bookbaskets, "absolute")
summary(bookCount)
```

10 livres les plus fréquents :

```{r 2.3.2.4 , warning=FALSE,message=FALSE}
orderedBooks <- sort(bookCount, decreasing = TRUE)
knitr::kable(orderedBooks[1:10])

#Fréquence d'apparition dans les paniers du livre le plus populaire
orderedBooks[1] / nrow(bookbaskets)
```

Clients ayant mis dans leur panier au moins 2 livres :

```{r 2.3.2.5 , warning=FALSE,message=FALSE}
bookbaskets_use <- bookbaskets[basketSizes > 1]
dim(bookbaskets_use)
```

Utilisation de la fonction apriori() :

```{r 2.3.2.6 , warning=FALSE,message=FALSE}
rules <- apriori(bookbaskets_use,
                  parameter = list(support = 0.002, confidence = 0.75))

summary(rules)
```

Inspection et evaluation des règles :

```{r 2.3.2.7 , warning=FALSE,message=FALSE}
measures <- interestMeasure(rules,
                  measure=c("coverage", "fishersExactTest"),
                  transactions = bookbaskets_use)
summary(measures)
```

Récupération des 5 règles avec le niveau de confiance le plus élevé :

```{r 2.3.2.8 , warning=FALSE,message=FALSE}
library(magrittr)

rules %>%
    sort(., by = "confidence") %>%
    head(., n = 5) %>%
    inspect(.)
```

Elaboration de règles avec des restrictions :

```{r 2.3.2.9 , warning=FALSE,message=FALSE}

# par défaut tous les livres peuvent se trouver dans la partie gauche (lhs) de la règle
# seul "The Lovely Bones" peut apparaître dans la partie droite (rhs)
brules <- apriori(bookbaskets_use,
parameter = list(support = 0.001,
                confidence = 0.6),
                appearance = list(rhs = c("The Lovely Bones: A Novel"),
                default = "lhs"))
summary(brules)
```

Inspection des règes :

```{r 2.3.2.10 , warning=FALSE,message=FALSE}
brules %>%
    sort(., by = "confidence") %>%
    lhs(.) %>%
    head(., n = 5) %>%
    inspect(.)
```

Inspection des règes avec restrictions :

```{r 2.3.2.11 , warning=FALSE,message=FALSE}
brulesSub <- subset(brules, subset = !(lhs %in% "Lucky : A Memoir"))

brulesSub %>%
        sort(., by = "confidence") %>%
        lhs(.) %>%
        head(., n = 5) %>%
        inspect(.)

brulesConf <- sort(brulesSub, by="confidence")

inspect(head(lhs(brulesConf), n = 5))
```
